{
 REMOVED_SECRETcellsREMOVED_SECRET: [
  {
   REMOVED_SECRETcell_typeREMOVED_SECRET: REMOVED_SECRETcodeREMOVED_SECRET,
   REMOVED_SECRETexecution_countREMOVED_SECRET: 1,
   REMOVED_SECRETmetadataREMOVED_SECRET: {},
   REMOVED_SECREToutputsREMOVED_SECRET: [],
   REMOVED_SECRETsourceREMOVED_SECRET: [
    REMOVED_SECRETimport torch\nREMOVED_SECRET,
    REMOVED_SECRETfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nREMOVED_SECRET,
    REMOVED_SECRETimport argparseREMOVED_SECRET
   ]
  },
  {
   REMOVED_SECRETcell_typeREMOVED_SECRET: REMOVED_SECRETcodeREMOVED_SECRET,
   REMOVED_SECRETexecution_countREMOVED_SECRET: 5,
   REMOVED_SECRETmetadataREMOVED_SECRET: {},
   REMOVED_SECREToutputsREMOVED_SECRET: [],
   REMOVED_SECRETsourceREMOVED_SECRET: [
    REMOVED_SECRETlang_code_map = {\nREMOVED_SECRET,
    REMOVED_SECRET    \REMOVED_SECRETen\REMOVED_SECRET: \REMOVED_SECRETen_XX\REMOVED_SECRET,\nREMOVED_SECRET,
    REMOVED_SECRET    \REMOVED_SECRETes\REMOVED_SECRET: \REMOVED_SECRETes_XX\REMOVED_SECRET,\nREMOVED_SECRET,
    REMOVED_SECRET    \REMOVED_SECRETit\REMOVED_SECRET: \REMOVED_SECRETit_IT\REMOVED_SECRET, \nREMOVED_SECRET,
    REMOVED_SECRET    \REMOVED_SECRETcn\REMOVED_SECRET: \REMOVED_SECRETzh_CN\REMOVED_SECRET\nREMOVED_SECRET,
    REMOVED_SECRET}\nREMOVED_SECRET,
    REMOVED_SECRET\nREMOVED_SECRET,
    REMOVED_SECRET\nREMOVED_SECRET,
    REMOVED_SECRETdef load_model_and_tokenizer(model_path):\nREMOVED_SECRET,
    REMOVED_SECRET    \REMOVED_SECRET\REMOVED_SECRET\REMOVED_SECRETLoad the model and tokenizer from the saved checkpoint\REMOVED_SECRET\REMOVED_SECRET\REMOVED_SECRET\nREMOVED_SECRET,
    REMOVED_SECRET    model = MBartForConditionalGeneration.from_pretrained(model_path)\nREMOVED_SECRET,
    REMOVED_SECRET    tokenizer = MBart50TokenizerFast.from_pretrained(model_path)\nREMOVED_SECRET,
    REMOVED_SECRET    return model, tokenizer\nREMOVED_SECRET,
    REMOVED_SECRET\nREMOVED_SECRET,
    REMOVED_SECRETdef translate_sentence(model, tokenizer, text, src_lang, tgt_lang):\nREMOVED_SECRET,
    REMOVED_SECRET    \REMOVED_SECRET\REMOVED_SECRET\REMOVED_SECRETTranslate a single sentence\REMOVED_SECRET\REMOVED_SECRET\REMOVED_SECRET\nREMOVED_SECRET,
    REMOVED_SECRET    # Set source and target languages\nREMOVED_SECRET,
    REMOVED_SECRET    tokenizer.src_lang = lang_code_map[src_lang]\nREMOVED_SECRET,
    REMOVED_SECRET    \nREMOVED_SECRET,
    REMOVED_SECRET    # Tokenize input\nREMOVED_SECRET,
    REMOVED_SECRET    inputs = tokenizer(text, return_tensors=\REMOVED_SECRETpt\REMOVED_SECRET, padding=True, truncation=True, max_length=64)\nREMOVED_SECRET,
    REMOVED_SECRET    \nREMOVED_SECRET,
    REMOVED_SECRET    # Generate translation\nREMOVED_SECRET,
    REMOVED_SECRET    with torch.no_grad():\nREMOVED_SECRET,
    REMOVED_SECRET        outputs = model.generate(\nREMOVED_SECRET,
    REMOVED_SECRET            **inputs,\nREMOVED_SECRET,
    REMOVED_SECRET            forced_bos_token_id=tokenizer.lang_code_to_id[lang_code_map[tgt_lang]],\nREMOVED_SECRET,
    REMOVED_SECRET            max_length=64,\nREMOVED_SECRET,
    REMOVED_SECRET            num_beams=4,\nREMOVED_SECRET,
    REMOVED_SECRET            early_stopping=True,\nREMOVED_SECRET,
    REMOVED_SECRET            no_repeat_ngram_size=3,  # Prevent repeating n-grams\nREMOVED_SECRET,
    REMOVED_SECRET            repetition_penalty=2.0,   # Penalize repetition\nREMOVED_SECRET,
    REMOVED_SECRET            length_penalty=1.0,       # Balance between length and score\nREMOVED_SECRET,
    REMOVED_SECRET            temperature=0.7,          # Control randomness\nREMOVED_SECRET,
    REMOVED_SECRET            do_sample=True           # Enable sampling\nREMOVED_SECRET,
    REMOVED_SECRET        )\nREMOVED_SECRET,
    REMOVED_SECRET\nREMOVED_SECRET,
    REMOVED_SECRET     # Decode the output\nREMOVED_SECRET,
    REMOVED_SECRET    translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\nREMOVED_SECRET,
    REMOVED_SECRET    return translation\nREMOVED_SECRET,
    REMOVED_SECRET\nREMOVED_SECRET,
    REMOVED_SECRETdef translate_file(model, tokenizer, input_file, output_file, src_lang, tgt_lang):\nREMOVED_SECRET,
    REMOVED_SECRET    \REMOVED_SECRET\REMOVED_SECRET\REMOVED_SECRETTranslate sentences from a file\REMOVED_SECRET\REMOVED_SECRET\REMOVED_SECRET\nREMOVED_SECRET,
    REMOVED_SECRET    with open(input_file, 'r', encoding='utf-8') as f_in, \\\nREMOVED_SECRET,
    REMOVED_SECRET         open(output_file, 'w', encoding='utf-8') as f_out:\nREMOVED_SECRET,
    REMOVED_SECRET        for line in f_in:\nREMOVED_SECRET,
    REMOVED_SECRET            text = line.strip()\nREMOVED_SECRET,
    REMOVED_SECRET            if text:  # Skip empty lines\nREMOVED_SECRET,
    REMOVED_SECRET                translation = translate_sentence(model, tokenizer, text, src_lang, tgt_lang)\nREMOVED_SECRET,
    REMOVED_SECRET                f_out.write(f\REMOVED_SECRET{translation}\\n\REMOVED_SECRET)REMOVED_SECRET
   ]
  },
  {
   REMOVED_SECRETcell_typeREMOVED_SECRET: REMOVED_SECRETcodeREMOVED_SECRET,
   REMOVED_SECRETexecution_countREMOVED_SECRET: 3,
   REMOVED_SECRETmetadataREMOVED_SECRET: {},
   REMOVED_SECREToutputsREMOVED_SECRET: [],
   REMOVED_SECRETsourceREMOVED_SECRET: [
    REMOVED_SECRETmodel, tokenizer = load_model_and_tokenizer(\REMOVED_SECRET./final\REMOVED_SECRET)REMOVED_SECRET
   ]
  },
  {
   REMOVED_SECRETcell_typeREMOVED_SECRET: REMOVED_SECRETcodeREMOVED_SECRET,
   REMOVED_SECRETexecution_countREMOVED_SECRET: 6,
   REMOVED_SECRETmetadataREMOVED_SECRET: {},
   REMOVED_SECREToutputsREMOVED_SECRET: [
    {
     REMOVED_SECRETdataREMOVED_SECRET: {
      REMOVED_SECRETtext/plainREMOVED_SECRET: [
       REMOVED_SECRET'Sports shoes'REMOVED_SECRET
      ]
     },
     REMOVED_SECRETexecution_countREMOVED_SECRET: 6,
     REMOVED_SECRETmetadataREMOVED_SECRET: {},
     REMOVED_SECREToutput_typeREMOVED_SECRET: REMOVED_SECRETexecute_resultREMOVED_SECRET
    }
   ],
   REMOVED_SECRETsourceREMOVED_SECRET: [
    REMOVED_SECRETtranslate_sentence(model, tokenizer, \REMOVED_SECRET运动鞋\REMOVED_SECRET, \REMOVED_SECRETcn\REMOVED_SECRET, \REMOVED_SECRETen\REMOVED_SECRET)REMOVED_SECRET
   ]
  },
  {
   REMOVED_SECRETcell_typeREMOVED_SECRET: REMOVED_SECRETcodeREMOVED_SECRET,
   REMOVED_SECRETexecution_countREMOVED_SECRET: 8,
   REMOVED_SECRETmetadataREMOVED_SECRET: {},
   REMOVED_SECREToutputsREMOVED_SECRET: [
    {
     REMOVED_SECRETdataREMOVED_SECRET: {
      REMOVED_SECRETtext/plainREMOVED_SECRET: [
       REMOVED_SECRET'運動鞋'REMOVED_SECRET
      ]
     },
     REMOVED_SECRETexecution_countREMOVED_SECRET: 8,
     REMOVED_SECRETmetadataREMOVED_SECRET: {},
     REMOVED_SECREToutput_typeREMOVED_SECRET: REMOVED_SECRETexecute_resultREMOVED_SECRET
    }
   ],
   REMOVED_SECRETsourceREMOVED_SECRET: [
    REMOVED_SECRETtranslate_sentence(model, tokenizer, \REMOVED_SECRETSports Shoes\REMOVED_SECRET, \REMOVED_SECRETen\REMOVED_SECRET, \REMOVED_SECRETcn\REMOVED_SECRET)REMOVED_SECRET
   ]
  }
 ],
 REMOVED_SECRETmetadataREMOVED_SECRET: {
  REMOVED_SECRETkernelspecREMOVED_SECRET: {
   REMOVED_SECRETdisplay_nameREMOVED_SECRET: REMOVED_SECRETtensorflow_baseREMOVED_SECRET,
   REMOVED_SECRETlanguageREMOVED_SECRET: REMOVED_SECRETpythonREMOVED_SECRET,
   REMOVED_SECRETnameREMOVED_SECRET: REMOVED_SECRETpython3REMOVED_SECRET
  },
  REMOVED_SECRETlanguage_infoREMOVED_SECRET: {
   REMOVED_SECRETcodemirror_modeREMOVED_SECRET: {
    REMOVED_SECRETnameREMOVED_SECRET: REMOVED_SECRETipythonREMOVED_SECRET,
    REMOVED_SECRETversionREMOVED_SECRET: 3
   },
   REMOVED_SECRETfile_extensionREMOVED_SECRET: REMOVED_SECRET.pyREMOVED_SECRET,
   REMOVED_SECRETmimetypeREMOVED_SECRET: REMOVED_SECRETtext/x-pythonREMOVED_SECRET,
   REMOVED_SECRETnameREMOVED_SECRET: REMOVED_SECRETpythonREMOVED_SECRET,
   REMOVED_SECRETnbconvert_exporterREMOVED_SECRET: REMOVED_SECRETpythonREMOVED_SECRET,
   REMOVED_SECRETpygments_lexerREMOVED_SECRET: REMOVED_SECRETipython3REMOVED_SECRET,
   REMOVED_SECRETversionREMOVED_SECRET: REMOVED_SECRET3.10.11REMOVED_SECRET
  }
 },
 REMOVED_SECRETnbformatREMOVED_SECRET: 4,
 REMOVED_SECRETnbformat_minorREMOVED_SECRET: 2
}
