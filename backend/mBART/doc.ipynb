{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9ae35d",
   "metadata": {},
   "source": [
    "# Overview of MT Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4ff736",
   "metadata": {},
   "source": [
    "### Step 1: Query Expansion CODE LOCATED IN queryexpansion.py\n",
    "We first use DeepSeek v3 to carry out query expansion on our queries. This is implemented as a function in queryexpansion.py, but will be demonstrated here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab33f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "#load API key\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "api_key = os.getenv('deepseek_API_KEY')\n",
    "\n",
    "#set up connection\n",
    "client = OpenAI(api_key=api_key, base_url=\"https://openrouter.ai/api/v1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c12d4",
   "metadata": {},
   "source": [
    "We define a function to expand our query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fe9f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expanded_queries(user_query):\n",
    "    prompt=f'''You are an expert search query optimizer. Your task is to expand the following e-commerce search query to improve retrieval of relevant products. Generate a list of semantically related terms, synonyms, and common user variations while preserving the original intent.\n",
    "\n",
    "**Rules:**\n",
    "1. Prioritize **contextual relevance** (e.g., \"running shoes\" → \"jogging sneakers\").\n",
    "2. Include **common misspellings** (e.g., \"earbuds\" → \"airbuds\").\n",
    "3. Add **technical/layman variants** (e.g., \"4K TV\" → \"ultra HD television\").\n",
    "4. For non-English queries, provide **translations/transliterations** if applicable (e.g., \"スマホ\" → \"smartphone\").\n",
    "5. Output in JSON format for easy parsing.\n",
    "\n",
    "**Input Query:** \"{user_query}\"\n",
    "\n",
    "**Output Format:**  \n",
    "{{\n",
    "  \"original_query\": \"...\",\n",
    "  \"expanded_terms\": [\n",
    "    {{\"term\": \"...\", \"type\": \"synonym\"}},\n",
    "    {{\"term\": \"...\", \"type\": \"misspelling\"}},\n",
    "    {{\"term\": \"...\", \"type\": \"technical\"}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "**Example Output for \"wireless headphones\":**\n",
    "{{\n",
    "  \"original_query\": \"wireless headphones\",\n",
    "  \"expanded_terms\": [\n",
    "    {{\"term\": \"Bluetooth headphones\", \"type\": \"synonym\"}},\n",
    "    {{\"term\": \"cordless earphones\", \"type\": \"synonym\"}},\n",
    "    {{\"term\": \"wireless headsets\", \"type\": \"synonym\"}},\n",
    "    {{\"term\": \"airbuds\", \"type\": \"misspelling\"}},\n",
    "    {{\"term\": \"noise-cancelling headphones\", \"type\": \"technical\"}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "**Now process this query:** \"{user_query}\"'''\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "    )\n",
    "    \n",
    "    expanded_queries_raw=response.choices[0].message.content\n",
    "    if not expanded_queries_raw or expanded_queries_raw.strip() == \"\":\n",
    "      raise ValueError(\"API returned an empty response\")\n",
    "    expanded_queries_raw = re.search(r'```json\\n({.*?})\\n```', expanded_queries_raw, re.DOTALL)\n",
    "    if expanded_queries_raw:\n",
    "      expanded_queries_raw = expanded_queries_raw.group(1)\n",
    "    else:\n",
    "      expanded_queries_raw = expanded_queries_raw.strip()  # fallback to raw response\n",
    "      \n",
    "    #print(expanded_queries_raw)\n",
    "    expanded_queries=json.loads(expanded_queries_raw)\n",
    "    return expanded_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2840849e",
   "metadata": {},
   "source": [
    "This query should return us an expanded version of the user's original query, accounting for misspellings, vague queries, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eafbc2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_query': 'shir long sleeve',\n",
       " 'expanded_terms': [{'term': 'shirt long sleeve', 'type': 'misspelling'},\n",
       "  {'term': 'long sleeve shirt', 'type': 'synonym'},\n",
       "  {'term': 'long sleeve t-shirt', 'type': 'synonym'},\n",
       "  {'term': 'long sleeve blouse', 'type': 'synonym'},\n",
       "  {'term': 'long sleeve top', 'type': 'synonym'},\n",
       "  {'term': 'long sleeve tee', 'type': 'synonym'},\n",
       "  {'term': 'long sleeve polo', 'type': 'synonym'},\n",
       "  {'term': 'long sleeve button-up', 'type': 'synonym'},\n",
       "  {'term': 'long sleeve dress shirt', 'type': 'synonym'},\n",
       "  {'term': 'long sleeve casual shirt', 'type': 'synonym'},\n",
       "  {'term': 'long sleeve henley', 'type': 'technical'},\n",
       "  {'term': 'long sleeve oxford shirt', 'type': 'technical'},\n",
       "  {'term': 'long sleeve flannel shirt', 'type': 'technical'},\n",
       "  {'term': 'long sleeve thermal shirt', 'type': 'technical'},\n",
       "  {'term': 'long sleeve knit shirt', 'type': 'technical'},\n",
       "  {'term': 'shirt long sleeved', 'type': 'misspelling'},\n",
       "  {'term': 'long sleeved shir', 'type': 'misspelling'},\n",
       "  {'term': 'long sleeved tshirt', 'type': 'misspelling'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"shir long sleeve\"\n",
    "#demo with misspelling\n",
    "expanded_queries=get_expanded_queries(query)\n",
    "\n",
    "expanded_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d7700",
   "metadata": {},
   "source": [
    "We then rank these expanded queries based on their types, giving the most importance to the original query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64dd2c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight the different output types\n",
    "def assign_weights(term_type):\n",
    "    weights = {\n",
    "        \"synonym\": 0.8,\n",
    "        \"misspelling\": 0.3,\n",
    "        \"technical\": 0.7,\n",
    "        \"translation\": 0.6\n",
    "    }\n",
    "    return weights.get(term_type, 0.5)  #default weight\n",
    "\n",
    "def return_weighted_dict(expanded_queries, include_translations): #option to remove translations for certain pipelines\n",
    "    weighted_terms = [\n",
    "    {\"term\": expanded_queries[\"original_query\"], \"weight\": 1.0}  # Original query (highest priority)\n",
    "    ]\n",
    "\n",
    "    if include_translations:\n",
    "      for item in expanded_queries[\"expanded_terms\"]:\n",
    "          weighted_terms.append({\n",
    "              \"term\": item[\"term\"],\n",
    "              \"weight\": assign_weights(item[\"type\"])\n",
    "          })\n",
    "    else:\n",
    "       for item in expanded_queries[\"expanded_terms\"]:\n",
    "          if item[\"type\"]!=\"translation\":\n",
    "            weighted_terms.append({\n",
    "                \"term\": item[\"term\"],\n",
    "                \"weight\": assign_weights(item[\"type\"])\n",
    "            })\n",
    "    return weighted_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eae29f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'term': 'shir long sleeve', 'weight': 1.0},\n",
       " {'term': 'shirt long sleeve', 'weight': 0.3},\n",
       " {'term': 'long sleeve shirt', 'weight': 0.8},\n",
       " {'term': 'long sleeve t-shirt', 'weight': 0.8},\n",
       " {'term': 'long sleeve blouse', 'weight': 0.8},\n",
       " {'term': 'long sleeve top', 'weight': 0.8},\n",
       " {'term': 'long sleeve tee', 'weight': 0.8},\n",
       " {'term': 'long sleeve polo', 'weight': 0.8},\n",
       " {'term': 'long sleeve button-up', 'weight': 0.8},\n",
       " {'term': 'long sleeve dress shirt', 'weight': 0.8},\n",
       " {'term': 'long sleeve casual shirt', 'weight': 0.8},\n",
       " {'term': 'long sleeve henley', 'weight': 0.7},\n",
       " {'term': 'long sleeve oxford shirt', 'weight': 0.7},\n",
       " {'term': 'long sleeve flannel shirt', 'weight': 0.7},\n",
       " {'term': 'long sleeve thermal shirt', 'weight': 0.7},\n",
       " {'term': 'long sleeve knit shirt', 'weight': 0.7},\n",
       " {'term': 'shirt long sleeved', 'weight': 0.3},\n",
       " {'term': 'long sleeved shir', 'weight': 0.3},\n",
       " {'term': 'long sleeved tshirt', 'weight': 0.3}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#demo with the above expansions\n",
    "weighted_queries=return_weighted_dict(expanded_queries, include_translations=False)\n",
    "weighted_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff99aeca",
   "metadata": {},
   "source": [
    "### Step 2: Fine-tuning of mBART model\n",
    "We first fine-tune an mBART model on our spanish, italian and chinese dataset to carry out our machine translation task. The code for fine-tuning can be found at finetune.py, while the model is saved in ./final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d7643",
   "metadata": {},
   "source": [
    "### Step 3: Machine Translation of expanded queries\n",
    "We then translate these queries using our finetuned mBART model. Similarly, this is implemented in translate.py but showcased here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97a7137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "lang_code_map = {\n",
    "    \"en\": \"en_XX\",\n",
    "    \"es\": \"es_XX\",\n",
    "    \"it\": \"it_IT\", \n",
    "    \"cn\": \"zh_CN\"\n",
    "}\n",
    "\n",
    "#function to load model and tokenizer\n",
    "def load_model_and_tokenizer(model_path):\n",
    "    \"\"\"Load the model and tokenizer from the saved checkpoint\"\"\"\n",
    "    model = MBartForConditionalGeneration.from_pretrained(model_path)\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "#translation function.\n",
    "def translate_sentence(model, tokenizer, text, src_lang, tgt_lang):\n",
    "    \"\"\"Translate a single sentence\"\"\"\n",
    "    # Set source and target languages\n",
    "    tokenizer.src_lang = lang_code_map[src_lang]\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "    \n",
    "    # Generate translation\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.lang_code_to_id[lang_code_map[tgt_lang]],\n",
    "            max_length=64,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,  # Prevent repeating n-grams\n",
    "            repetition_penalty=2.0,   # Penalize repetition\n",
    "            length_penalty=1.0,       # Balance between length and score\n",
    "            temperature=0.7,          # Control randomness\n",
    "            do_sample=True           # Enable sampling\n",
    "        )\n",
    "\n",
    "     # Decode the output\n",
    "    translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    return translation\n",
    "\n",
    "def translate_expanded(model, tokenizer, query_list, src_lang, tgt_lang):\n",
    "    for query in query_list:\n",
    "        query['term']=translate_sentence(model, tokenizer, query['term'], src_lang, tgt_lang)\n",
    "    return query_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ab51c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#demo using expanded queries\n",
    "tgt_lang='cn'\n",
    "model, tokenizer = load_model_and_tokenizer(\"./final\")\n",
    "weighted_queries = translate_expanded(model, tokenizer, weighted_queries, 'en', tgt_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d70da652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'term': '雪龍袖', 'weight': 1.0},\n",
       " {'term': '衬衫長袖', 'weight': 0.3},\n",
       " {'term': '長袖衬衫', 'weight': 0.8},\n",
       " {'term': '長袖t恤衫', 'weight': 0.8},\n",
       " {'term': '長袖 blouse', 'weight': 0.8},\n",
       " {'term': '長袖上衣', 'weight': 0.8},\n",
       " {'term': '長袖 tee', 'weight': 0.8},\n",
       " {'term': '長袖保羅', 'weight': 0.8},\n",
       " {'term': '長袖上扣', 'weight': 0.8},\n",
       " {'term': '長袖裙衫', 'weight': 0.8},\n",
       " {'term': '長袖休闲衣', 'weight': 0.8},\n",
       " {'term': '長袖亨利', 'weight': 0.7},\n",
       " {'term': '長袖牛仔衫', 'weight': 0.7},\n",
       " {'term': '長袖flannel衬衫', 'weight': 0.7},\n",
       " {'term': '長袖熱帶衫', 'weight': 0.7},\n",
       " {'term': '長袖編織衬衫', 'weight': 0.7},\n",
       " {'term': '衬衫長袖', 'weight': 0.3},\n",
       " {'term': '長袖雪子', 'weight': 0.3},\n",
       " {'term': '長袖tshirt', 'weight': 0.3}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e461d7",
   "metadata": {},
   "source": [
    "### Step 4: Hybrid Search of expanded queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29f5054",
   "metadata": {},
   "source": [
    "#### 4.1 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34946e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a dicitonary of dfs\n",
    "import pandas as pd\n",
    "\n",
    "def get_data(data_paths):\n",
    "    data = {} \n",
    "    for lang, path in data_paths.items():\n",
    "        data[lang]=pd.read_pickle(path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfe4b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths={'cn':'en_to_cn_embeddings.pkl', 'es':'en_to_sp_embeddings.pkl', 'it':'en_to_it_embeddings.pkl'}\n",
    "data = get_data(data_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ccbfa",
   "metadata": {},
   "source": [
    "#### BM25 Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84d6a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import pandas as pd\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "febd1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build BM_25 corpus\n",
    "def build_BM25(data):\n",
    "    #cn\n",
    "    entocn_chinese_titles = data['cn']['chinese translation']\n",
    "    entocn_tokenized_cn = [list(jieba.cut_for_search(title.lower())) for title in entocn_chinese_titles]\n",
    "    bm25_cn = BM25Okapi(entocn_tokenized_cn)\n",
    "\n",
    "    #es\n",
    "    entoes_spanish_titles = data['es']['title_spanish']\n",
    "    entoes_tokenized_es = [title.split() for title in entoes_spanish_titles]\n",
    "    bm25_es = BM25Okapi(entoes_tokenized_es)\n",
    "\n",
    "    #it\n",
    "    entoit_italian_titles = data['it']['title_italian']\n",
    "    entoit_tokenized_it = [title.split() for title in entoit_italian_titles]\n",
    "    bm25_it = BM25Okapi(entoit_tokenized_it)\n",
    "\n",
    "    bm25_corpus={'cn':bm25_cn, 'es':bm25_es, 'it':bm25_it}\n",
    "\n",
    "\n",
    "    return bm25_corpus\n",
    "\n",
    "\n",
    "#Search BM25\n",
    "def search_bm25_expanded(query_list, corpus, tgt_lang='cn', top_k=5):\n",
    "    #init scores as zeros\n",
    "\n",
    "    scores = [0.0] * len(corpus[tgt_lang].doc_len)\n",
    "\n",
    "    for query_dict in query_list:\n",
    "        term=query_dict['term']\n",
    "        weight=query_dict['weight']\n",
    "        if tgt_lang=='cn':\n",
    "            tokens=jieba.cut_for_search(term.lower())\n",
    "            term_scores = corpus[tgt_lang].get_scores(tokens)        \n",
    "        else:\n",
    "            tokens = term.lower().split()\n",
    "            term_scores = corpus[tgt_lang].get_scores(tokens)\n",
    "\n",
    "        scores = [s + weight * ts for s, ts in zip(scores, term_scores)]\n",
    "\n",
    "    # Get top-k ranked indices\n",
    "    top_k_ids = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "    return top_k_ids, [scores[i] for i in top_k_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b556de8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\liuru\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.491 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "bm25_corpus = build_BM25(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c4f4687",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ids, top_scores = search_bm25_expanded(weighted_queries, bm25_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcceebfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141.3012 | Polo Shirt Classic Denim Pocket Long Sleeve Shirt  | Polo衫 經典丹寧 口袋長袖襯衫\n",
      "112.8441 | Kids Set Table Bay - Thin Long Sleeve Home Suit Magic Baby ~ K60092  | 兒童套裝 台灣製薄長袖居家套裝 魔法Baby~k60092\n",
      "88.6328 | GAP Kids Long Sleeve Logo Patch Top  | GAP 童裝長袖Logo貼布上衣\n",
      "71.6278 | Long version Pocket Cardigan Knit Coat  | 長版口袋開襟針織外套\n",
      "65.9903 | IFairies large size long sleeve T shirt Tops ifairies [59000] 【 59000 】  | iFairies 中大尺碼長袖T恤上衣★ifairies【59000】【59000】\n"
     ]
    }
   ],
   "source": [
    "#remember our original search was 'shir long sleeve', mispelled on purpose.\n",
    "\n",
    "for i, score in zip(top_ids, top_scores):\n",
    "    print(f\"{score:.4f} | {data['cn']['title'][i]}  | {data['cn']['chinese translation'][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f994cf4",
   "metadata": {},
   "source": [
    "#### Dense Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df47839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"BAAI/bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e0843e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "#Embeds a dense embedding representing the weighted mean of the expanded queries\n",
    "def embed_expanded(query_list, model):\n",
    "    query_embeddings= []\n",
    "    #embed expanded queries\n",
    "    for query_dict in query_list:\n",
    "        embedding=model.encode(query_dict['term'],  convert_to_tensor=True).cpu().numpy() #size1024\n",
    "        query_embeddings.append(embedding * query_dict[\"weight\"])\n",
    "\n",
    "    query_embedding = sum(query_embeddings) / len(query_embeddings)  # Weighted mean\n",
    "    return query_embedding\n",
    "\n",
    "\n",
    "def init_index(pc, index_name, data, embedding_col, eng_col, tgt_col, tgt_lang):\n",
    "    index_name = index_name\n",
    "    dimension = 1024\n",
    "\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=dimension,\n",
    "            metric=\"cosine\",  # by cosine similarity\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",  # or \"gcp\"\n",
    "                region=\"us-east-1\" \n",
    "            )\n",
    "        )\n",
    "\n",
    "    index = pc.Index(index_name)\n",
    "\n",
    "    vectors_to_upsert = []\n",
    "    for _, row in data.iterrows():\n",
    "        vectors_to_upsert.append({\n",
    "            \"id\": str(_),  # Use index or generate unique IDs\n",
    "            \"values\": row[embedding_col],  # Using Chinese embeddings\n",
    "            \"metadata\": {\n",
    "                \"title\": row[eng_col],\n",
    "                \"chinese_title\": row[tgt_col],\n",
    "                \"embedding_type\": tgt_lang  # Track which embedding was used\n",
    "            }\n",
    "        })\n",
    "\n",
    "    for i in range(0, len(vectors_to_upsert), 100):\n",
    "        index.upsert(vectors=vectors_to_upsert[i:i+100])\n",
    "\n",
    "def setup_pinecone(data):\n",
    "    load_dotenv(dotenv_path='../.env')\n",
    "    pinecone_api_key = os.getenv('pinecone_API_KEY')\n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "    data = data\n",
    "    \n",
    "    indexes={'cn':'cn-search', 'it':'it-search', 'es':'es-search'}\n",
    "\n",
    "    #setup cn\n",
    "    init_index(pc, index_name=indexes['cn'], data=data['cn'],\n",
    "     embedding_col='chinese_embedding',\n",
    "     eng_col='title',\n",
    "     tgt_col='chinese translation',\n",
    "     tgt_lang='chinese')\n",
    "\n",
    "    #setup it\n",
    "    init_index(pc, index_name=indexes['it'], data=data['it'],\n",
    "     embedding_col='italian_embedding',\n",
    "     eng_col='title',\n",
    "     tgt_col='title_italian',\n",
    "     tgt_lang='italian')\n",
    "\n",
    "    #setup es\n",
    "    init_index(pc, index_name=indexes['es'], data=data['es'],\n",
    "     embedding_col='spanish_embedding',\n",
    "     eng_col='title',\n",
    "     tgt_col='title_spanish',\n",
    "     tgt_lang='spanish')\n",
    "\n",
    "    return indexes\n",
    "\n",
    "def search_pinecone(query_list, embedding_model, index_name, top_k=5):\n",
    "    load_dotenv(dotenv_path='../.env')\n",
    "    pinecone_api_key = os.getenv('pinecone_API_KEY')\n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "    index = pc.Index(index_name)\n",
    "    query_embedding=embed_expanded(query_list, embedding_model)\n",
    "    results = index.query(\n",
    "            vector=query_embedding.tolist(),\n",
    "            top_k=top_k,\n",
    "            include_metadata=False\n",
    "        )\n",
    "    id_list = []\n",
    "    score_list = []\n",
    "    for dict in results.matches:\n",
    "        id_list.append(int(dict['id']))\n",
    "        score_list.append(float(dict['score']))\n",
    "\n",
    "    return id_list, score_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f7a2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_indices=setup_pinecone(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0efad315",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ids_pc, top_scores_pc =search_pinecone(weighted_queries, model, pinecone_indices['cn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0cf11e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7519 | Long version Pocket Cardigan Knit Coat  | 長版口袋開襟針織外套\n",
      "0.7019 | Polo Shirt Classic Denim Pocket Long Sleeve Shirt  | Polo衫 經典丹寧 口袋長袖襯衫\n",
      "0.6916 | Shiny Glossy Long Edition Perspective Shirt  | 閃亮光澤長版透視襯衫\n",
      "0.6759 | Korean Made. Thick Straps Cross Vest  | 韩制。粗肩带交叉背心\n",
      "0.6581 | IFairies large size long sleeve T shirt Tops ifairies [59000] 【 59000 】  | iFairies 中大尺碼長袖T恤上衣★ifairies【59000】【59000】\n"
     ]
    }
   ],
   "source": [
    "#remember our original search was 'shir long sleeve', mispelled on purpose.\n",
    "\n",
    "for i, score in zip(top_ids_pc, top_scores_pc):\n",
    "    print(f\"{score:.4f} | {data['cn']['title'][i]}  | {data['cn']['chinese translation'][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7690b596",
   "metadata": {},
   "source": [
    "#### RRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a085a8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[656, 996, 701, 276, 418] [141.30117982812962, 112.84411227054272, 88.63276428673784, 71.6278067714302, 65.99026659028007]\n"
     ]
    }
   ],
   "source": [
    "#recap: Right now, we have BM25 results, returned as\n",
    "print(top_ids, top_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b655385f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[276, 656, 134, 76, 418] [0.7518996, 0.701878309, 0.691553712, 0.675868392, 0.658072412]\n"
     ]
    }
   ],
   "source": [
    "#recap: We also have semantic results, returned as\n",
    "print(top_ids_pc, top_scores_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a460d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def scores_to_ranking(scores: list[float]) -> list[int]:\n",
    "    \"\"\"Convert float scores into int rankings (1 = best).\"\"\"\n",
    "    return np.argsort(scores)[::-1] + 1  # ranks start at 1\n",
    "\n",
    "def rrf(keyword_rank: int, semantic_rank: int, k: int = 60) -> float:\n",
    "    \"\"\"Combine keyword rank and semantic rank into a hybrid score using RRF.\"\"\"\n",
    "    return 1 / (k + keyword_rank) + 1 / (k + semantic_rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4b0b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_expanded_search(query_list, bm25_corpus, pinecone_indices, embedding_model, tgt_lang='cn', top_k=5 ):\n",
    "    bm25_top_ids, bm25_top_scores = search_bm25_expanded(query_list, bm25_corpus, top_k=top_k)\n",
    "    pc_top_ids, pc_top_scores =search_pinecone(query_list, embedding_model, pinecone_indices[tgt_lang], top_k=top_k)\n",
    "    bm25_ranks = scores_to_ranking(bm25_top_scores)\n",
    "    pc_ranks = scores_to_ranking(pc_top_scores)\n",
    "\n",
    "    # Create dictionaries for quick rank lookup\n",
    "    bm25_rank_dict = {doc_id: rank for doc_id, rank in zip(bm25_top_ids, bm25_ranks)}\n",
    "    pc_rank_dict = {doc_id: rank for doc_id, rank in zip(pc_top_ids, pc_ranks)}\n",
    "    \n",
    "    # Combine all unique document IDs from both methods\n",
    "    all_doc_ids = list(set(bm25_top_ids) | set(pc_top_ids))\n",
    "    \n",
    "    # Calculate RRF scores for each document\n",
    "    rrf_scores = []\n",
    "    for doc_id in all_doc_ids:\n",
    "        # Get ranks from each method (use a high rank if document not found)\n",
    "        bm25_rank = bm25_rank_dict.get(doc_id, top_k * 2)  # Penalize missing documents\n",
    "        pc_rank = pc_rank_dict.get(doc_id, top_k * 2)\n",
    "        \n",
    "        # Calculate combined RRF score\n",
    "        score = rrf(bm25_rank, pc_rank)\n",
    "        rrf_scores.append((doc_id, score))\n",
    "    \n",
    "    # Sort documents by RRF score (descending)\n",
    "    rrf_scores.sort(key=lambda x: -x[1])\n",
    "    \n",
    "    # Extract the top_k document IDs\n",
    "    #hybrid_top_ids = [doc_id for doc_id, score in rrf_scores[:top_k]]\n",
    "    hybrid_top_ids = [doc_id for doc_id, score in rrf_scores]\n",
    "\n",
    "    #hybrid_top_scores = [score for doc_id, score in rrf_scores[:top_k]]\n",
    "    hybrid_top_scores = [score for doc_id, score in rrf_scores]\n",
    "    \n",
    "    return hybrid_top_ids, hybrid_top_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "43bd0e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_top_id, hybrid_top_scores=hybrid_expanded_search(weighted_queries, bm25_corpus, pinecone_indices, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3cf19797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0325 | Polo Shirt Classic Denim Pocket Long Sleeve Shirt  | Polo衫 經典丹寧 口袋長袖襯衫\n",
      "0.0320 | Long version Pocket Cardigan Knit Coat  | 長版口袋開襟針織外套\n",
      "0.0308 | IFairies large size long sleeve T shirt Tops ifairies [59000] 【 59000 】  | iFairies 中大尺碼長袖T恤上衣★ifairies【59000】【59000】\n",
      "0.0304 | Kids Set Table Bay - Thin Long Sleeve Home Suit Magic Baby ~ K60092  | 兒童套裝 台灣製薄長袖居家套裝 魔法Baby~k60092\n",
      "0.0302 | Shiny Glossy Long Edition Perspective Shirt  | 閃亮光澤長版透視襯衫\n",
      "0.0302 | GAP Kids Long Sleeve Logo Patch Top  | GAP 童裝長袖Logo貼布上衣\n",
      "0.0299 | Korean Made. Thick Straps Cross Vest  | 韩制。粗肩带交叉背心\n"
     ]
    }
   ],
   "source": [
    "for i, score in zip(hybrid_top_id, hybrid_top_scores):\n",
    "    print(f\"{score:.4f} | {data['cn']['title'][i]}  | {data['cn']['chinese translation'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "48db3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "import warnings\n",
    "\n",
    "def calculate_bertscore(candidate, reference, lang = \"en\"):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        # Compute scores\n",
    "        P, R, F1 = score(\n",
    "            [candidate], \n",
    "            [reference], \n",
    "            lang=lang,\n",
    "            model_type=\"bert-base-multilingual-cased\",  # Multilingual BERT\n",
    "            verbose=False  # Disable progress messages\n",
    "        )\n",
    "    return P.item(), R.item(), F1.item()\n",
    "\n",
    "\n",
    "def get_final_output(query, hybrid_top_id, data, tgt_lang='cn'):\n",
    "    final_output={}\n",
    "    for ids in hybrid_top_id:\n",
    "        if tgt_lang=='cn':\n",
    "            txt=data[tgt_lang]['chinese translation'][ids]\n",
    "        elif tgt_lang=='es':\n",
    "            txt=data[tgt_lang]['title_spanish'][ids]\n",
    "        elif tgt_lang=='it':\n",
    "            txt=data[tgt_lang]['title_italian'][ids]\n",
    "\n",
    "        acc, precision, f1 = calculate_bertscore(txt, query)\n",
    "        final_output[txt]=f1\n",
    "    return final_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a8a504f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"shir long sleeve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9f01fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "final_output = get_final_output(query, hybrid_top_id, data, tgt_lang='cn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5c580dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Polo衫 經典丹寧 口袋長袖襯衫': 0.6623846292495728,\n",
       " '長版口袋開襟針織外套': 0.6716357469558716,\n",
       " 'iFairies 中大尺碼長袖T恤上衣★ifairies【59000】【59000】': 0.6212053894996643,\n",
       " '兒童套裝 台灣製薄長袖居家套裝 魔法Baby~k60092': 0.6144145727157593,\n",
       " '閃亮光澤長版透視襯衫': 0.6694958806037903,\n",
       " 'GAP 童裝長袖Logo貼布上衣': 0.659843921661377,\n",
       " '韩制。粗肩带交叉背心': 0.655915379524231}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0872cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import argparse\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from translate import load_model_and_tokenizer, translate_sentence\n",
    "from queryexpansion import expand\n",
    "from langdetect import detect\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da627ba2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2893126158.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    for query in\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def mtpipeline(input_query, tgt_lang=\"cn\"):\n",
    "    load_dotenv(dotenv_path='..../.env')\n",
    "    model_path = os.getenv('mBART_path')\n",
    "    model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "    input_query_expanded=expand(input_query, include_translations=False)\n",
    "    for query in input_query_expanded:\n",
    "        query['term']=translate_sentence(model, tokenizer, query['term'], 'en', tgt_lang)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2546c887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
